# hpml-peft-benchmark
Benchmarking parameter-efficient fine-tuning (PEFT) methods for large language models. Compares full fine-tuning, LoRA, and QLoRA across GPU memory, latency, and accuracy. Includes training scripts, profiling tools, and reproducible configs for Columbia HPML Fall 2025.
